{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder Model\n",
    "This model is not suitable for Autocorrelated Time Series Data with lags (The PM2.5 dataset may not fit very well). While it is more commonly used in the image domain, the encoder's output can effectively represent the correlations between variables in a two-dimensional plane. The dimensionality reduction method employed is t-SNE, which is somewhat similar to PCA but has the advantage of capturing nonlinear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.nn as nn\n",
    "import random \n",
    "import os \n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import textwrap\n",
    "from sklearn.manifold import TSNE\n",
    "# Parameter configuration\n",
    "args = {\n",
    "    \"corpusFile\": \"./ground_pm25.csv\",  # Path to the data file\n",
    "    \"gpu\": 0,  # GPU device index\n",
    "    \"epochs\": 100,  # Number of training epochs\n",
    "    \"input_size\": 8,  # Dimension of input features\n",
    "    \"encoding_size\": 2,  # Dimension of output of the encoder\n",
    "    \"lr\": 0.0008,  # Learning rate\n",
    "    \"batch_size\": 64,  # Batch size\n",
    "    \"useGPU\": True,  # Whether to use GPU\n",
    "    \"batch_first\": True,  # Whether to set batch_size as the first dimension\n",
    "    \"dropout\": 0.005,  # Dropout rate\n",
    "    \"model_name\": \"AutoEncoder\",  # Model name\n",
    "    \"output_size\": 1,  # Dimension of output features\n",
    "}\n",
    "\n",
    "# Automatically generate the model save path\n",
    "args[\"save_file\"] = f\"model/{args['model_name'].lower()}.pth\"\n",
    "\n",
    "# Device selection\n",
    "device = torch.device(f\"cuda:{args['gpu']}\" if torch.cuda.is_available() and args['useGPU'] else \"cpu\")\n",
    "args[\"device\"] = device\n",
    "\n",
    "# Print configuration parameters\n",
    "print(\"Configuration Parameters:\")\n",
    "for k, v in args.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)  # Convert features to PyTorch tensors\n",
    "        self.Y = torch.tensor(Y, dtype=torch.float32)  # Convert targets to PyTorch tensors\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples.\"\"\"\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Retrieve the feature-target pair at the specified index.\"\"\"\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "\n",
    "def get_data(corpusFile, batch_size):\n",
    "    df = pd.read_csv(corpusFile, index_col=0)\n",
    "    \n",
    "    # Store the maximum and minimum values for each column for later scaling\n",
    "    df_max = df.max()\n",
    "    df_min = df.min()\n",
    "    \n",
    "    # Initialize the MinMaxScaler for feature scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler_Y = MinMaxScaler()\n",
    "    # Separate the target variable 'pm25' from the features\n",
    "    Y = df['pm25'].values  # Extract target values\n",
    "    X = df.drop('pm25', axis=1).values  # Extract feature values\n",
    "    \n",
    "    # Fit the scaler on the features and transform them\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    Y_scaled = scaler_Y.fit_transform(Y.reshape(-1, 1))\n",
    "    \n",
    "    # Determine the lengths for training, validation, and testing splits\n",
    "    total_len = len(X_scaled)\n",
    "    train_len = int(total_len * 0.8)  # 80% for training\n",
    "    test_size = int(0.1 * total_len)  # 10% for testing\n",
    "    val_size = total_len - train_len - test_size  # Remaining 10% for validation\n",
    "    \n",
    "    # Split the data into training, validation, and testing sets\n",
    "    train_x, train_y = X_scaled[:train_len], Y_scaled[:train_len]\n",
    "    val_x, val_y = X_scaled[train_len:train_len + val_size], Y_scaled[train_len:train_len + val_size]\n",
    "    test_x, test_y = X_scaled[train_len + val_size:], Y_scaled[train_len + val_size:]\n",
    "    \n",
    "    # Create DataLoaders for each dataset split\n",
    "    train_loader = DataLoader(\n",
    "        dataset=AutoEncoderDataset(train_x, train_y),\n",
    "        batch_size=args['batch_size'],\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        dataset=AutoEncoderDataset(val_x, val_y),\n",
    "        batch_size=args['batch_size'],\n",
    "        shuffle=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=AutoEncoderDataset(test_x, test_y),\n",
    "        batch_size=args['batch_size'],\n",
    "        shuffle=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "    return train_loader, val_loader, test_loader, df_max, df_min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self,input_size,output_size,encoding_size,dropout):\n",
    "        super(AutoEncoder,self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128,64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64,encoding_size),\n",
    "            nn.BatchNorm1d(encoding_size),\n",
    "            nn.ReLU(True),\n",
    "            )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_size,64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128,output_size),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "    def forward(self,x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        encoded_shape = encoded.shape\n",
    "        decoded_shape = decoded.shape\n",
    "        return encoded,decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seed \n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  #\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "\n",
    "#define train function\n",
    "def train(): \n",
    "    set_seed()  # Set the random seed for reproducibility\n",
    "\n",
    "    # Initialize the LSTM model with parameters from the args dictionary\n",
    "    model = AutoEncoder(\n",
    "        input_size=args['input_size'],      # Number of expected features in the input\n",
    "        output_size=args['output_size'],        # Output size \n",
    "        encoding_size=args['encoding_size'],    # Size of the encoded vector\n",
    "        dropout=args['dropout']             # Dropout probability\n",
    "    )\n",
    "    model.to(args['device'])  # Move the model to the specified device (CPU or GPU)\n",
    "\n",
    "    criterion = nn.MSELoss()  # Define the loss function (Mean Squared Error)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])  # Initialize the optimizer\n",
    "\n",
    "    # Load and preprocess the data\n",
    "    labels_max, labels_min, train_loader, val_loader, test_loader = get_data(\n",
    "        args['corpusFile'],\n",
    "        args['batch_size']\n",
    "    )\n",
    "\n",
    "    epoch_losses = []  # To store training losses per epoch\n",
    "    val_losses = []    # To store validation losses per epoch\n",
    "    best_val_loss = float('inf')  # Initialize the best validation loss\n",
    "\n",
    "    for epoch in range(args['epochs']):\n",
    "        model.train()  # Set the model to training mode\n",
    "        epoch_loss = 0\n",
    "        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{args[\"epochs\"]}', leave=False)\n",
    "\n",
    "        # Training loop\n",
    "        for inputs, targets in train_bar:\n",
    "            inputs, targets = inputs.to(args['device']), targets.to(args['device'])  # Move data to device\n",
    "            encoded, decoded = model(inputs)  # Forward pass\n",
    "            loss = criterion(decoded, targets)  # Compute loss\n",
    "\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            loss.backward()        # Backward pass\n",
    "            optimizer.step()       # Update parameters\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            train_bar.set_postfix(loss=loss.item())  # Update progress bar with current loss\n",
    "        avg_train_loss = epoch_loss /len(train_loader)\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0\n",
    "\n",
    "        # Validation loop without gradient computation\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in val_loader:\n",
    "                \n",
    "                val_inputs, val_targets = val_inputs.to(args['device']), val_targets.to(args['device'])\n",
    "                val_encoded, val_decoded = model(val_inputs)  # Forward pass\n",
    "                val_loss += criterion(val_decoded, val_targets).item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)  # Calculate average validation loss\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Print training and validation losses\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{args[\"epochs\"]}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}') \n",
    "        \n",
    "        # Log the losses to a file\n",
    "        with open(f'{args[\"model_name\"]}_loss.txt', 'a') as f:\n",
    "            f.write(f'Epoch {epoch+1}, Train Loss: {epoch_loss:.4f}, Val Loss: {avg_val_loss:.4f}\\n')\n",
    "\n",
    "        # Save the model if validation loss has improved\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            save_dir = os.path.dirname(args['save_file'])\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({'state_dict': model.state_dict()}, args['save_file'])\n",
    "    print(f'Best model saved at epoch {epoch+1}')\n",
    "        \n",
    "        # Ensure the model is back in training mode for the next epoch\n",
    "    model.train()\n",
    "    \n",
    "train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on the Test Dataset and TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(args):\n",
    "\n",
    "    model = AutoEncoder(\n",
    "        input_size=args['input_size'],\n",
    "        output_size=args['output_size'],\n",
    "        encoding_size=args['encoding_size'],\n",
    "        dropout=args['dropout']\n",
    "    )\n",
    "    model.to(args['device'])\n",
    "    \n",
    "\n",
    "    checkpoint = torch.load(args['save_file'], map_location=args['device'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "\n",
    "    train_loader, val_loader, test_loader, df_max, df_min, scaler_X, scaler_Y = get_data(\n",
    "        args['corpusFile'], \n",
    "        args['batch_size']\n",
    "    )\n",
    "    \n",
    "    all_encoded = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(args['device'])\n",
    "            encoded, decoded = model(inputs)  \n",
    "            all_encoded.append(encoded.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    \n",
    "\n",
    "    encoded_features = np.vstack(all_encoded) \n",
    "    targets = np.vstack(all_targets)        \n",
    "\n",
    "    if args['encoding_size'] > 2:\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        encoded_2d = tsne.fit_transform(encoded_features)  \n",
    "    else:\n",
    "        encoded_2d = encoded_features  \n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(encoded_2d[:, 0], encoded_2d[:, 1], c=targets.squeeze(), cmap='viridis', alpha=0.5)\n",
    "    plt.colorbar(scatter, label='PM2.5')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.title('Encoder Output in 2D Space')\n",
    "    plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "  \n",
    "    save_plot_dir = './plot'\n",
    "    os.makedirs(save_plot_dir, exist_ok=True)\n",
    "    \n",
    " \n",
    "    plt.savefig(f'{save_plot_dir}/{args[\"model_name\"]}_encoder_output.png', dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_table(data, title):\n",
    "    \n",
    "    df = pd.DataFrame(data).T\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.index += 1 \n",
    "    df.index.name = 'Index' \n",
    "    def wrap_text(text, width):\n",
    "        return '\\n'.join(textwrap.wrap(text, width))\n",
    "\n",
    "    max_width = 30\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: wrap_text(str(x), max_width))\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 5), dpi=300)\n",
    "    ax.set_title(title, fontsize=16, fontweight='bold', color='Black') \n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "\n",
    "    table = ax.table(cellText=df.values,\n",
    "                     rowLabels=df.index,\n",
    "                     colLabels=df.columns,\n",
    "                     cellLoc='center',\n",
    "                     loc='center',\n",
    "                     rowLoc='center',\n",
    "                     bbox=[0, 0, 1, 1])\n",
    "\n",
    "\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    table.scale(1, 1)\n",
    "\n",
    "    num_rows = len(df) + 1 \n",
    "    num_cols = len(df.columns) + 1  \n",
    "\n",
    "    for (row, col), cell in table.get_celld().items():\n",
    "        cell.visible_edges = ''\n",
    "        if row == 0:\n",
    "            \n",
    "            cell.get_text().set_fontweight('bold')\n",
    "            cell.set_height(0.05)\n",
    "            cell.visible_edges = 'BT'\n",
    "            cell.set_edgecolor('black')\n",
    "            cell.set_linewidth(1)\n",
    "            cell.get_text().set_color('blue')\n",
    "            cell.set_facecolor('#d3d3d3')\n",
    "        elif col == 0:\n",
    "        \n",
    "            cell.get_text().set_fontweight('bold')\n",
    "            \n",
    "            cell.set_facecolor('#ecf8fd')\n",
    "        else:\n",
    "\n",
    "            if row % 2 == 1:\n",
    "                cell.set_facecolor('#f9f9f9')\n",
    "            else:\n",
    "                cell.set_facecolor('#ffffff')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "create_table()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
